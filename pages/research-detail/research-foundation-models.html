<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Foundation Models - JC STEM Lab</title>
    <link rel="icon" type="image/png" href="/assets/favicon.png">
    <link rel="stylesheet" href="../../styles.css">
    <link rel="stylesheet" href="research-detail.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="foundation-theme">
    <div class="back-nav">
        <div class="container">
            <a href="../../index.html#research">
                <i class="fas fa-arrow-left"></i> Back to Research Areas
            </a>
        </div>
    </div>

    <header class="research-header">
        <div class="container">
            <i class="fas fa-brain header-icon"></i>
            <h1>Foundation Models</h1>
            <p class="subtitle">Two complementary thrusts: LLM agentic reasoning and systems for complex tasks; World Models for Spatial Intelligence as the cognitive core for Embodied AI (e.g., autonomous vehicles).</p>
        </div>
    </header>

    <main class="research-content">
        <div class="container">
            <div class="research-section">
                <h2><i class="fas fa-robot"></i> LLM</h2>
                
                <div class="research-subsection">
                    <h4>Agentic Reasoning and Reinforcement Learning</h4>
                    <ul>
                        <li>Designing agentic reasoning and reinforcement learning methods to improve the performance of LLMs.</li>
                        <li>Exploring how LLMs can iteratively plan, reflect, and act in complex environments through chain-of-thought and self-correction mechanisms.</li>
                        <li>Integrating reinforcement learning (RLHF, RLAIF) to align LLM outputs with human preferences and task objectives.</li>
                    </ul>
                </div>
                
                <div class="research-subsection">
                    <h4>Agentic Systems</h4>
                    <ul>
                        <li>Designing agentic systems (single/multi-agent) that can achieve complex tasks.</li>
                        <li>Building single-agent systems that autonomously decompose goals, use tools, and execute multi-step plans.</li>
                        <li>Developing multi-agent frameworks for collaboration, negotiation, and emergent problem-solving in open-ended environments.</li>
                    </ul>
                </div>
                
                <div class="research-subsection">
                    <h4>PEFT and Task Adaptation</h4>
                    <ul>
                        <li>Designing PEFT methods and task-adaptation strategies to adapt LLMs to specific tasks better and more efficiently.</li>
                        <li>Low-rank adaptation (LoRA), adapter modules, and prompt tuning for parameter-efficient fine-tuning.</li>
                        <li>Task-specific adaptation for embodied AI, robotics, and domain-specific applications with limited compute.</li>
                    </ul>
                </div>
            </div>

            <div class="research-section">
                <h2><i class="fas fa-globe"></i> World Model</h2>
                
                <div class="research-subsection">
                    <h4>Spatial Intelligence for Embodied AI</h4>
                    <ul>
                        <li>Constructing a comprehensive World Model for Spatial Intelligence, serving as the cognitive core for Embodied AI (e.g., Autonomous Vehicles).</li>
                        <li>The system mirrors the current physical world and reasons about its dynamics—enabling agents to predict outcomes and plan actions.</li>
                        <li>Unifying perception, prediction, and planning within a coherent World Model for safe and intelligent autonomous behavior.</li>
                    </ul>
                </div>
                
                <div class="research-subsection">
                    <h4>Spatial Breadth — Collaborative Perception</h4>
                    <ul>
                        <li>Collaborative Perception among agents to overcome individual sensing limitations (e.g., occlusions, limited range).</li>
                        <li>This collective intelligence builds a more complete and robust World Model, ensuring agents maintain accurate 3D understanding.</li>
                        <li>Addressing challenges in complex, adversarial, or bandwidth-constrained environments through efficient information sharing and fusion.</li>
                    </ul>
                </div>
                
                <div class="research-subsection">
                    <h4>Temporal Depth — Generative Models and 3D Reconstruction</h4>
                    <ul>
                        <li>Leveraging Generative Models and 3D Gaussian Splatting to empower agents to hallucinate plausible future scenarios.</li>
                        <li>Recovering 3D geometry from sparse observations for scene completion and understanding.</li>
                        <li>This predictive capability enables the World Model to simulate potential outcomes, facilitating foresightful planning and decision-making for Embodied AI.</li>
                    </ul>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2024 JC STEM Lab of Smart City, City University of Hong Kong. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
